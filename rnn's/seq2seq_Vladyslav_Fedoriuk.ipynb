{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "http://www.statmt.org/wmt20/translation-task.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    DATASET_SIZE = 4000\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 40\n",
    "    MAX_FEATURES = 100\n",
    "    EMBEDDING_DIM = 256\n",
    "    HIDDEN_DIM = 300\n",
    "    MAX_SEQ_LEN = 50\n",
    "    TEST_SIZE = 0.2\n",
    "    VAL_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 4000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "texts_en = []\n",
    "texts_pl = []\n",
    "\n",
    "with open('en-pl.txt') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i>=Configuration.DATASET_SIZE:\n",
    "            break\n",
    "        en, pl = tuple(line.split('\\t'))\n",
    "        re.sub(r'[\\t\\n]', '', pl)\n",
    "\n",
    "        texts_pl.append('\\t ' + pl.strip() + ' \\n')\n",
    "        texts_en.append(en.strip())\n",
    "\n",
    "\n",
    "len(texts_en), len(texts_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score big bonuses by matching 3 in a row, and being fast.Controls: Click on the hats with the mouse to uncover the pumpkin underneath.Find the identical pairs of pumpkins Rating: 4, Votes: 2, Played 8951 times , Size: (887.451 kb)concentration games, funny games, halloween games, pumpkin games, memory games, brain training games, swapping games, hat games, spooky games, Play free online games, computer flash games.\n",
      "\t Wynik duże premie, dopasowując 3 w rzędzie, i jest szybki.kontroli Instrukcje: kliknij na kapelusze z myszką, aby odkryć dyni pod spodem. znaleźć identyczne pary dyni , Size: popularność: 4, głosów: 2, Grano 8966 Rrazy (887.451 kb)Stężenie gry, zabawny gry, halloween gry, dynia gry, pamięci gry, szkolenia mózgu gry, zamiana gry, kapelusz gry, Spooky gry, grać bezpłatny online Flash gry szt. gry 2009-xx, Free Online PC Games .NET , Privacy, Contact us \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2560, 640, 800)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "en_train_data, en_test_data, pl_train_data, pl_test_data = train_test_split(\n",
    "    texts_en, \n",
    "    texts_pl,\n",
    "    test_size=Configuration.TEST_SIZE,\n",
    "    random_state=42\n",
    ")\n",
    "en_train_data, en_val_data, pl_train_data, pl_val_data = train_test_split(\n",
    "    en_train_data,\n",
    "    pl_train_data,\n",
    "    test_size=Configuration.TEST_SIZE,\n",
    "    random_state=42\n",
    ")\n",
    "print(en_test_data[0])\n",
    "print(pl_test_data[0])\n",
    "len(en_train_data), len(en_val_data), len(en_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13178, 18255)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer_en = Tokenizer(\n",
    "    num_words=Configuration.MAX_FEATURES,\n",
    ")\n",
    "tokenizer_pl = Tokenizer(\n",
    "    num_words=Configuration.MAX_FEATURES,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    ")\n",
    "\n",
    "tokenizer_en.fit_on_texts(texts_en)\n",
    "tokenizer_pl.fit_on_texts(texts_pl)\n",
    "\n",
    "en_word_indices = tokenizer_en.word_index\n",
    "pl_word_indices = tokenizer_pl.word_index\n",
    "len(en_word_indices), len(pl_word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_word_indices.get('\\t'), pl_word_indices.get('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train_data = tokenizer_en.texts_to_sequences(en_train_data)\n",
    "en_val_data = tokenizer_en.texts_to_sequences(en_val_data)\n",
    "pl_train_data = tokenizer_pl.texts_to_sequences(pl_train_data)\n",
    "pl_val_data = tokenizer_pl.texts_to_sequences(pl_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "en_train_data = pad_sequences(\n",
    "    sequences=en_train_data,\n",
    "    maxlen=Configuration.MAX_SEQ_LEN,\n",
    "    padding='post',\n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "en_val_data = pad_sequences(\n",
    "    sequences=en_val_data,\n",
    "    maxlen=Configuration.MAX_SEQ_LEN,\n",
    "    padding='post',\n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "pl_train_data = pad_sequences(\n",
    "    sequences=pl_train_data,\n",
    "    maxlen=Configuration.MAX_SEQ_LEN,\n",
    "    padding='post',\n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "pl_val_data = pad_sequences(\n",
    "    sequences=pl_val_data,\n",
    "    maxlen=Configuration.MAX_SEQ_LEN,\n",
    "    padding='post',\n",
    "    truncating='post'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building an encoder\n",
    "\n",
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, LSTM\n",
    "\n",
    "encoder_input = Input(shape=(None, ), dtype='int32')\n",
    "encoder_embedding = Embedding(\n",
    "    input_dim = Configuration.MAX_FEATURES + 1,\n",
    "    output_dim = Configuration.EMBEDDING_DIM,\n",
    "    # input_length = Configuration.MAX_SEQ_LEN - 1\n",
    ")\n",
    "encoder_lstm = LSTM(\n",
    "    Configuration.HIDDEN_DIM,\n",
    "    return_state = True\n",
    ")\n",
    "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding(encoder_input))\n",
    "encoder = Model(\n",
    "    inputs=encoder_input, \n",
    "    outputs=[encoder_state_h, encoder_state_c]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a decoder\n",
    "from keras.layers import TimeDistributed, Dense\n",
    "\n",
    "decoder_input = Input(shape=(None, ), dtype='int32')\n",
    "state_h_input = Input(shape=(Configuration.HIDDEN_DIM, ), dtype='float32')\n",
    "state_c_input = Input(shape=(Configuration.HIDDEN_DIM, ), dtype='float32')\n",
    "decoder_embedding = Embedding(\n",
    "    input_dim = Configuration.MAX_FEATURES + 1,\n",
    "    output_dim = Configuration.EMBEDDING_DIM,\n",
    "    # input_length = Configuration.MAX_SEQ_LEN\n",
    ")\n",
    "decoder_lstm = LSTM(\n",
    "    Configuration.HIDDEN_DIM,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    ")\n",
    "decoder_fc = TimeDistributed(Dense(Configuration.MAX_FEATURES, activation='softmax'))\n",
    "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(\n",
    "    decoder_embedding(decoder_input),\n",
    "    initial_state=[\n",
    "        state_h_input,\n",
    "        state_c_input\n",
    "    ]\n",
    ")\n",
    "decoder_output = decoder_fc(decoder_outputs)\n",
    "decoder = Model(\n",
    "    inputs=[\n",
    "        decoder_input,\n",
    "        state_h_input,\n",
    "        state_c_input\n",
    "    ],\n",
    "    outputs=[\n",
    "        decoder_output,\n",
    "        decoder_state_h,\n",
    "        decoder_state_c\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder-decoder model\n",
    "_outputs, _, _ = decoder_lstm(\n",
    "    decoder_embedding(decoder_input),\n",
    "    initial_state=[\n",
    "        encoder_state_h,\n",
    "        encoder_state_c\n",
    "    ]\n",
    ")\n",
    "_prob_outputs = decoder_fc(_outputs)\n",
    "encoder_decoder = Model(\n",
    "    inputs=[\n",
    "        encoder_input,\n",
    "        decoder_input\n",
    "    ],\n",
    "    outputs=_prob_outputs\n",
    ")\n",
    "encoder_decoder.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2560, 49), (640, 49))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target sequence take the shifted words\n",
    "import numpy as np\n",
    "pl_train_data_target = pl_train_data[:,1:]\n",
    "pl_val_data_target = pl_val_data[:,1:]\n",
    "# remove the last word from original input sequence to align with target sequence\n",
    "pl_train_data = pl_train_data[:,:-1]\n",
    "pl_val_data = pl_val_data[:, :-1]\n",
    "\n",
    "# one-hot vectorization\n",
    "# one_hot_train_target = tokenizer_pl.sequences_to_matrix(pl_train_data_target, mode='binary')\n",
    "# one_hot_val_target = tokenizer_pl.sequences_to_matrix(pl_val_data_target, mode='binary')\n",
    "\n",
    "def one_hot_vectorize(data, seq_len, dict_size):\n",
    "    one_hot = np.zeros((len(data), seq_len, dict_size))\n",
    "    for i, seq in enumerate(data):\n",
    "        for j, word_idx in enumerate(seq):\n",
    "            one_hot[i,j,word_idx]=1\n",
    "    return one_hot\n",
    "\n",
    "one_hot_train_target = one_hot_vectorize(\n",
    "    pl_train_data_target,\n",
    "    Configuration.MAX_SEQ_LEN - 1,\n",
    "    Configuration.MAX_FEATURES\n",
    ")\n",
    "one_hot_val_target = one_hot_vectorize(\n",
    "    pl_val_data_target,\n",
    "    Configuration.MAX_SEQ_LEN - 1,\n",
    "    Configuration.MAX_FEATURES\n",
    ")\n",
    "pl_train_data.shape, pl_val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 2.3607 - val_loss: 1.9524\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 1.7247 - val_loss: 1.5479\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 1.3567 - val_loss: 1.1994\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 14s 357ms/step - loss: 1.0421 - val_loss: 0.9108\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 13s 314ms/step - loss: 0.8207 - val_loss: 0.7412\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 13s 318ms/step - loss: 0.6867 - val_loss: 0.6415\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 13s 318ms/step - loss: 0.5996 - val_loss: 0.5641\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 13s 319ms/step - loss: 0.5373 - val_loss: 0.5142\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 0.4932 - val_loss: 0.4856\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 14s 350ms/step - loss: 0.4777 - val_loss: 0.5410\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 14s 358ms/step - loss: 0.4672 - val_loss: 0.4448\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 14s 358ms/step - loss: 0.4249 - val_loss: 0.4273\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 15s 367ms/step - loss: 0.4052 - val_loss: 0.4102\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 14s 346ms/step - loss: 0.3876 - val_loss: 0.3969\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 14s 357ms/step - loss: 0.3741 - val_loss: 0.3884\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 14s 351ms/step - loss: 0.3612 - val_loss: 0.3807\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 0.3512 - val_loss: 0.3709\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 0.3412 - val_loss: 0.3679\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 13s 330ms/step - loss: 0.3322 - val_loss: 0.3623\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 13s 318ms/step - loss: 0.3233 - val_loss: 0.3596\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 12s 306ms/step - loss: 0.3177 - val_loss: 0.3504\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 0.3088 - val_loss: 0.3481\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 12s 306ms/step - loss: 0.3056 - val_loss: 0.3567\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 13s 313ms/step - loss: 0.3039 - val_loss: 0.3400\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 12s 311ms/step - loss: 0.2923 - val_loss: 0.3390\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 13s 327ms/step - loss: 0.2853 - val_loss: 0.3344\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.2769 - val_loss: 0.3342\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 12s 292ms/step - loss: 0.2701 - val_loss: 0.3324\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.2642 - val_loss: 0.3295\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 12s 311ms/step - loss: 0.2589 - val_loss: 0.3285\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 15s 364ms/step - loss: 0.2569 - val_loss: 0.3313\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 14s 345ms/step - loss: 0.2493 - val_loss: 0.3215\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 13s 327ms/step - loss: 0.2420 - val_loss: 0.3238\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 12s 308ms/step - loss: 0.2373 - val_loss: 0.3242\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 0.2328 - val_loss: 0.3184\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 14s 348ms/step - loss: 0.2244 - val_loss: 0.3165\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 0.2202 - val_loss: 0.3177\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 15s 375ms/step - loss: 0.2188 - val_loss: 0.3237\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 14s 358ms/step - loss: 0.2115 - val_loss: 0.3266\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 14s 360ms/step - loss: 0.2056 - val_loss: 0.3191\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 15s 366ms/step - loss: 0.2005 - val_loss: 0.3221\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 15s 364ms/step - loss: 0.1940 - val_loss: 0.3207\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 0.1877 - val_loss: 0.3190\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 0.1831 - val_loss: 0.3185\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 14s 357ms/step - loss: 0.1802 - val_loss: 0.3271\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 16s 388ms/step - loss: 0.1745 - val_loss: 0.3206\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 15s 370ms/step - loss: 0.1683 - val_loss: 0.3182\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 14s 350ms/step - loss: 0.1629 - val_loss: 0.3178\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 15s 369ms/step - loss: 0.1593 - val_loss: 0.3233\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 14s 352ms/step - loss: 0.1562 - val_loss: 0.3231\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 0.1582 - val_loss: 0.3282\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 14s 353ms/step - loss: 0.1499 - val_loss: 0.3221\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 14s 345ms/step - loss: 0.1384 - val_loss: 0.3226\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 14s 360ms/step - loss: 0.1366 - val_loss: 0.3251\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 0.1336 - val_loss: 0.3233\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 0.1265 - val_loss: 0.3241\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 0.1222 - val_loss: 0.3236\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 14s 347ms/step - loss: 0.1191 - val_loss: 0.3294\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 16s 391ms/step - loss: 0.1191 - val_loss: 0.3393\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 15s 375ms/step - loss: 0.1177 - val_loss: 0.3323\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 15s 372ms/step - loss: 0.1110 - val_loss: 0.3317\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 16s 388ms/step - loss: 0.1059 - val_loss: 0.3310\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 15s 384ms/step - loss: 0.1041 - val_loss: 0.3333\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 14s 355ms/step - loss: 0.1030 - val_loss: 0.3385\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 15s 367ms/step - loss: 0.0988 - val_loss: 0.3420\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 15s 372ms/step - loss: 0.1027 - val_loss: 0.3458\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 14s 353ms/step - loss: 0.1018 - val_loss: 0.3403\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 15s 376ms/step - loss: 0.0958 - val_loss: 0.3410\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 14s 360ms/step - loss: 0.0926 - val_loss: 0.3441\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 14s 356ms/step - loss: 0.0866 - val_loss: 0.3478\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 14s 353ms/step - loss: 0.0812 - val_loss: 0.3450\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 12s 307ms/step - loss: 0.0788 - val_loss: 0.3493\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 16s 390ms/step - loss: 0.0754 - val_loss: 0.3526\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 15s 387ms/step - loss: 0.0736 - val_loss: 0.3511\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 15s 382ms/step - loss: 0.0705 - val_loss: 0.3519\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 15s 380ms/step - loss: 0.0679 - val_loss: 0.3537\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 17s 421ms/step - loss: 0.0665 - val_loss: 0.3600\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.0670 - val_loss: 0.3581\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 17s 437ms/step - loss: 0.0643 - val_loss: 0.3640\n",
      "Epoch 80/100\n",
      "21/40 [==============>...............] - ETA: 46s - loss: 0.0630"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_decoder.fit(\n",
    "    x=[en_train_data, pl_train_data], \n",
    "    y=one_hot_train_target,\n",
    "    batch_size=Configuration.BATCH_SIZE,\n",
    "    epochs=Configuration.EPOCHS, \n",
    "    validation_data=(\n",
    "        [en_val_data, pl_val_data],\n",
    "        one_hot_val_target\n",
    "    ),\n",
    "    validation_batch_size=Configuration.BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_state(sentence):\n",
    "    en_idx_word = tokenizer_en.index_word\n",
    "    en_word_idx = tokenizer_en.word_index\n",
    "    pl_idx_word = tokenizer_pl.index_word\n",
    "    pl_word_idx = tokenizer_pl.word_index\n",
    "    start_token_idx = pl_word_idx.get('\\t', None)\n",
    "    if not start_token_idx:\n",
    "        raise ValueError('there must be a start token')\n",
    "    \n",
    "    sequence = tokenizer_en.texts_to_sequences([sentence])\n",
    "    sequence = pad_sequences( \n",
    "        sequences=sequence,\n",
    "        maxlen=Configuration.MAX_SEQ_LEN,\n",
    "        padding='post',\n",
    "        truncating='post'\n",
    "    )\n",
    "    encoder_states = encoder.predict(sequence)\n",
    "    return ([start_token_idx], ['\\t'], [1.0]), encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state(token_triple, states):\n",
    "    en_idx_word = tokenizer_en.index_word\n",
    "    en_word_idx = tokenizer_en.word_index\n",
    "    pl_idx_word = tokenizer_pl.index_word\n",
    "    pl_word_idx = tokenizer_pl.word_index\n",
    "    \n",
    "    indices, tokens, probabs = token_triple\n",
    "    probs, state_h, state_c = decoder.predict([np.array([[indices[-1]]]), states[0], states[1]])\n",
    "    states_new = [state_h, state_c]\n",
    "    probs = probs[0, -1, :]\n",
    "    index_new = np.argmax(probs)\n",
    "    token_new = pl_idx_word.get(index_new, None)\n",
    "    prob_new = np.max(probs)\n",
    "    token_triple_new = (\n",
    "        indices + [index_new],\n",
    "        tokens + [token_new],\n",
    "        probabs + [prob_new]\n",
    "    )\n",
    "    return token_triple_new, states_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_lstm(sentence):\n",
    "    token_triple, states = initiate_state(sentence)\n",
    "    for _ in range(Configuration.MAX_SEQ_LEN):\n",
    "        token_triple, states = update_state(token_triple, states)\n",
    "        if token_triple[1][-1] == '\\n' or token_triple[1][-1] is None:\n",
    "            token_triple[1][-1] = ''\n",
    "            break\n",
    "    return ' '.join(token_triple[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: \n",
      "\t Friday, July 15, 2016 in Everett the weather will be like this: during the night the air temperature drops to +12...+14°C, dew point: +11,8°C; ratio of temperature, wind speed and humidity: Very comfortable; precipitation is not expected, light breeze wind blowing from the north-west at a speed of 7-11 km/h , clear sky; day length is 15:38\n",
      "Original translation: \n",
      "\t \t W piątek, 15 lipca 2016 w Everett charakter pogoda będzie taka: w nocy temperatura powietrza spada do +12...+14°C, punkt rosy: +11,8°C; wartości temperatury i wilgotności: bardzo dobrze; atmosferyczne nie ma, wiatr Słaby wiatr, podmuchowy z od północnego-zachodu z prędkością 2-3 m/s , niebo czyste; długość dnia 15:38 \n",
      "\n",
      "Seq2Seq translation: \n",
      "\t \t wpisz dane a my oddzwonimy do ciebie imię i nazwisko telefon zadaj pytanie imię i nazwisko email telefon pytanie oferta numer na sprzedaż dodano 2009 województwo kujawsko pomorskie \n",
      "Original sentence: \n",
      "\t See more What's New in Version 1.2.2(07/21/2017) Now Cart to PDF for Prestashop 1.7 About the developer list 8 Products developed today 04/25/2012 PrestaShop member flag Spain Country Captain Expertise Level Question?Need help?\n",
      "Original translation: \n",
      "\t \t Dowiedz się więcej Co nowego w wersji 1.2.2(2017-07-21) Now Cart to PDF for Prestashop 1.7 O wydawcy list 8 opracowanych produktów today 2012-04-25 Sprzedawca PrestaShop flag Spain Kraj region Captain Poziom ekspertyzy Masz pytanie? \n",
      "\n",
      "Seq2Seq translation: \n",
      "\t \t wpisz dane a my oddzwonimy do ciebie imię i nazwisko telefon zadaj pytanie imię i nazwisko email telefon pytanie oferta numer na sprzedaż dodano 2009 województwo kujawsko pomorskie \n",
      "Original sentence: \n",
      "\t At the same time, I want to inform you that if I receive a message like \"I thought you and your sister were […] Continue reading Margie has moved out! / A few more wedding memories… / Maroon 5! 2015-03-082018-11-26 Aga 2 Comments every day, life in the USA // POLISH VERSION HERE //Everything in my life changes so fast!\n",
      "Original translation: \n",
      "\t \t Jednocześnie chciałabym zaznaczyć, że jeśli dostaję komentarze czy wiadomości typu \"myślałam, że z siostrą macie dobre relacje\" (przykład pierwszy z brzegu) i nie ma […] Czytaj dalej Margie się wyprowadziła! / Trochę więcej wspomnień ślubnych… / Maroon 5! 2015-03-082018-11-26 Aga 58 komentarzy na co dzień, życie w usa // ENGLISH VERSION HERE //Wszystko w moim życiu zmienia się jak w kalejdoskopie! \n",
      "\n",
      "Seq2Seq translation: \n",
      "\t \t w lipca 2016 w charakter pogoda będzie taka temperatura powietrza spada do punkt rosy 12 wartości temperatury i wilgotności bardzo dobrze atmosferyczne nie ma wiatr wiatr podmuchowy z z z prędkością 2 3 m s niebo czyste długość dnia 15 \n"
     ]
    }
   ],
   "source": [
    "for i, (en_sentence, pl_sentence) in enumerate(zip(en_test_data, pl_test_data)):\n",
    "    translation = infer_lstm(en_sentence)\n",
    "    print(f'Original sentence: \\n\\t {en_sentence}')\n",
    "    print(f'Original translation: \\n\\t {pl_sentence}')\n",
    "    print(f'Seq2Seq translation: \\n\\t {translation}')\n",
    "    if i==2:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
