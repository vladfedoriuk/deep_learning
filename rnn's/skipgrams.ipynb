{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'saint-exupery-maly-ksiaze.txt'\n",
    "MAX_FEATURES = 3700\n",
    "EMBEDDING_DIM = 128\n",
    "WINDOW_SIZE = 10\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Antoine de Saint-Exupéry\\n',\n",
       " 'tłum. Agata Kozak\\n',\n",
       " 'Proszę dzieci, aby mi wybaczyły, że dedykuję tę książkę dorosłemu. Mam niebagatelne usprawiedliwienie: ten dorosły to mój najlepszy przyjaciel na świecie. Mam też inne usprawiedliwienie: ten dorosły potrafi wszystko zrozumieć, nawet książki dla dzieci. Mam jeszcze trzecie usprawiedliwienie: ten dorosły mieszka we Francji, gdzie cierpi głód i chłód. Bardzo potrzebuje pociechy. Jeżeli wszystkie te powody okażą się niewystarczające, chętnie zadedykuję tę książkę dziecku, którym był kiedyś ten dorosły. Wszyscy dorośli byli najpierw dziećmi. (Ale niewielu z nich o tym pamięta.) Poprawiam więc dedykację:\\n',\n",
       " 'z czasów, kiedy był małym chłopcem\\n',\n",
       " 'Kiedy miałem sześć lat, zobaczyłem pewnego razu wspaniały obrazek w książce o dżungli zatytułowanej Historie prawdziwe. Przedstawiał węża boa połykającego lwa. Oto kopia tego rysunku:\\n']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file_path) as file:\n",
    "    lines = file.readlines()\n",
    "lines = list(filter(lambda x: len(x.split(' ')) > 2, lines))\n",
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3704"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=MAX_FEATURES,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n—',\n",
    ")\n",
    "tokenizer.fit_on_texts(lines)\n",
    "word_index = tokenizer.word_index\n",
    "index_word = tokenizer.index_word\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 1, 128)       473728      input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 1, 128)       473728      input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_6 (Dot)                     (None, 1, 1)         0           embedding_12[0][0]               \n",
      "                                                                 embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1)            0           dot_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1)            0           reshape_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 947,456\n",
      "Trainable params: 947,456\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Dot, Reshape, Activation\n",
    "from keras import Model\n",
    "\n",
    "word_input = Input(shape=(1, ), dtype='int32')\n",
    "word_embedding = Embedding(\n",
    "    MAX_FEATURES + 1,\n",
    "    EMBEDDING_DIM\n",
    ")(word_input)\n",
    "\n",
    "context_input = Input(shape=(1, ), dtype='int32')\n",
    "context_embedding  = Embedding(\n",
    "    MAX_FEATURES + 1,\n",
    "    EMBEDDING_DIM,\n",
    ")(context_input)\n",
    "\n",
    "dot = Dot(axes=2)([word_embedding, context_embedding])\n",
    "dot = Reshape((1,), input_shape=(1, 1))(dot)\n",
    "activation = Activation('sigmoid')(dot)\n",
    "\n",
    "model = Model(inputs=[word_input, context_input], outputs=activation)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[703, 2604], [704, 705], [704, 1884], [704, 3006], [705, 1776]],\n",
       " [0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "X, y = [], []\n",
    "\n",
    "for sequence in sequences:\n",
    "    couples, labels = skipgrams(\n",
    "        sequence, \n",
    "        MAX_FEATURES+1, \n",
    "        window_size=WINDOW_SIZE, \n",
    "        negative_samples=1.0, \n",
    "        shuffle=True,\n",
    "        seed=42\n",
    "    )\n",
    "    X.extend(couples), y.extend(labels)\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[703],\n",
       "         [704]]),\n",
       "  array([[2604],\n",
       "         [ 705]])],\n",
       " array([[0],\n",
       "        [1]]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def data_generator(batch_size):\n",
    "    words, contexts, labels = [], [], []\n",
    "    while True:\n",
    "        for i , (couple, label) in enumerate(zip(X, y)):\n",
    "            word = np.array([couple[0]])\n",
    "            context = np.array([couple[1]])\n",
    "            words.append(word)\n",
    "            contexts.append(context)\n",
    "            labels.append([label])\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                yield [np.asarray(words), np.asarray(contexts)], np.asarray(labels)\n",
    "                words = []\n",
    "                contexts = []\n",
    "                labels = []\n",
    "\n",
    "next(data_generator(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10297/10296 [==============================] - 96s 9ms/step - loss: 0.5275 - accuracy: 0.7554\n",
      "Epoch 2/10\n",
      "10297/10296 [==============================] - 95s 9ms/step - loss: 0.4617 - accuracy: 0.8005\n",
      "Epoch 3/10\n",
      "10297/10296 [==============================] - 96s 9ms/step - loss: 0.3953 - accuracy: 0.8343\n",
      "Epoch 4/10\n",
      "10297/10296 [==============================] - 93s 9ms/step - loss: 0.3184 - accuracy: 0.8747\n",
      "Epoch 5/10\n",
      "10297/10296 [==============================] - 93s 9ms/step - loss: 0.2491 - accuracy: 0.9077\n",
      "Epoch 6/10\n",
      "10297/10296 [==============================] - 93s 9ms/step - loss: 0.1960 - accuracy: 0.9294\n",
      "Epoch 7/10\n",
      "10297/10296 [==============================] - 95s 9ms/step - loss: 0.1589 - accuracy: 0.9430\n",
      "Epoch 8/10\n",
      "10297/10296 [==============================] - 94s 9ms/step - loss: 0.1342 - accuracy: 0.9503\n",
      "Epoch 9/10\n",
      "10297/10296 [==============================] - 94s 9ms/step - loss: 0.1180 - accuracy: 0.9549\n",
      "Epoch 10/10\n",
      "10297/10296 [==============================] - 94s 9ms/step - loss: 0.1076 - accuracy: 0.9576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb77771cac0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = len(X)/BATCH_SIZE\n",
    "\n",
    "model.fit(\n",
    "    data_generator(BATCH_SIZE),\n",
    "    epochs = EPOCHS,\n",
    "    steps_per_epoch = steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.057225496,\n",
       " -0.20672044,\n",
       " -0.5354497,\n",
       " -0.23587765,\n",
       " 0.047212545,\n",
       " -0.13578546,\n",
       " 0.105006434,\n",
       " 0.2249624,\n",
       " 0.60114485,\n",
       " 0.011181073,\n",
       " 0.028876191,\n",
       " -0.2150197,\n",
       " -0.19229287,\n",
       " -0.42304903,\n",
       " -0.3870341,\n",
       " -0.26633513,\n",
       " 0.20992306,\n",
       " -0.26789796,\n",
       " 0.63235205,\n",
       " 0.18417272,\n",
       " -0.23758674,\n",
       " -0.086795725,\n",
       " 0.40518805,\n",
       " -0.26184282,\n",
       " 0.19725634,\n",
       " -0.08173382,\n",
       " 0.07478525,\n",
       " -0.32972407,\n",
       " -0.07596091,\n",
       " -0.1818065,\n",
       " -0.39132196,\n",
       " 0.12636523,\n",
       " -0.37860554,\n",
       " -0.09653058,\n",
       " 0.09106132,\n",
       " -0.28574094,\n",
       " 0.16958746,\n",
       " 0.3122325,\n",
       " 0.037383165,\n",
       " -0.17949788,\n",
       " -0.64787537,\n",
       " 0.3050659,\n",
       " 0.9507067,\n",
       " -0.07905616,\n",
       " 0.22398783,\n",
       " 0.59930265,\n",
       " -0.052212063,\n",
       " 0.10895566,\n",
       " 0.27813455,\n",
       " 0.17503741,\n",
       " 0.12873241,\n",
       " -0.24379854,\n",
       " -0.3741396,\n",
       " -0.9290059,\n",
       " -0.28661683,\n",
       " -0.18567401,\n",
       " -0.2175693,\n",
       " -0.13793968,\n",
       " -0.06619417,\n",
       " -0.21876732,\n",
       " 0.085079394,\n",
       " 0.4730686,\n",
       " -0.52085024,\n",
       " 0.13513213,\n",
       " -0.09339047,\n",
       " -0.141848,\n",
       " -0.12019018,\n",
       " -0.61028796,\n",
       " -0.103933506,\n",
       " -0.2973206,\n",
       " 0.46471804,\n",
       " -0.12984988,\n",
       " -0.044988733,\n",
       " -0.23443723,\n",
       " -0.27629504,\n",
       " 0.2259953,\n",
       " 0.23494543,\n",
       " -0.18015122,\n",
       " -0.2160229,\n",
       " -0.40430468,\n",
       " 0.31509486,\n",
       " 0.24008262,\n",
       " 0.1556946,\n",
       " -0.045340024,\n",
       " 0.45869473,\n",
       " 0.6624986,\n",
       " 0.32113966,\n",
       " -0.56482357,\n",
       " -0.471274,\n",
       " 0.26684874,\n",
       " 0.053843293,\n",
       " 0.32340327,\n",
       " 0.2333271,\n",
       " -0.43182516,\n",
       " 0.6486227,\n",
       " 0.64122456,\n",
       " 0.011929003,\n",
       " -0.22139198,\n",
       " -0.10528486,\n",
       " 0.4108425,\n",
       " 0.14273262,\n",
       " -0.05494356,\n",
       " -0.5156958,\n",
       " 0.05930652,\n",
       " 0.27791196,\n",
       " -0.2745573,\n",
       " -0.29206884,\n",
       " 0.91711926,\n",
       " -0.4151221,\n",
       " -0.6286975,\n",
       " 0.28174686,\n",
       " 0.31858662,\n",
       " 0.3524715,\n",
       " 0.009309565,\n",
       " 0.028157292,\n",
       " 0.17021671,\n",
       " -0.030730473,\n",
       " 0.3585093,\n",
       " -0.19178616,\n",
       " -0.17372096,\n",
       " -0.28905782,\n",
       " -0.3854714,\n",
       " -0.30173782,\n",
       " -0.31326935,\n",
       " 0.045686502,\n",
       " 0.1723986,\n",
       " -0.4339387,\n",
       " 0.3692184]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec={}\n",
    "vectors = model.get_weights()[0] # weights without bias\n",
    "for word, i in word_index.items():\n",
    "    if i < MAX_FEATURES:\n",
    "        word2vec[word]=list(vectors[i, :])\n",
    "\n",
    "word2vec['dzieci']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_vector(words, word2vec):\n",
    "    vec = np.zeros((EMBEDDING_DIM,), dtype=\"float32\")\n",
    "    for word in words:\n",
    "        if word in word2vec.keys():\n",
    "            vec = np.add(vec, word2vec[word])\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06781631708145142"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine, euclidean\n",
    "sentence1 = ['proszę', 'dzieci', 'aby', 'mi', 'wybaczyły']\n",
    "sentence2 = ['proszę', 'dorośli', 'aby', 'mi', 'wybaczyły']\n",
    "\n",
    "cosine(\n",
    "    avg_sentence_vector(\n",
    "        sentence1,\n",
    "        word2vec\n",
    "    ),\n",
    "      avg_sentence_vector(\n",
    "      sentence2,\n",
    "      word2vec\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.935206413269043"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean(\n",
    "    avg_sentence_vector(\n",
    "        sentence1,\n",
    "        word2vec\n",
    "    ),\n",
    "      avg_sentence_vector(\n",
    "      sentence2,\n",
    "      word2vec\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[484, 254, 709, 78, 257, 710, 209]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words = ['usprawiedliwienie', 'dorosły', 'potrafi', 'wszystko', 'zrozumieć', 'książki', 'dzieci']\n",
    "test_words_idx = [word_index[word] for word in test_words]\n",
    "test_words_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'usprawiedliwienie': [('trzecie', 0.2641702798591403),\n",
       "              ('niebagatelne', 0.2794349904318243),\n",
       "              ('wybaczyły', 0.3741950350028179),\n",
       "              ('chłód', 0.38417133248781477)],\n",
       "             'dorosły': [('poznał', 0.4007957122756466),\n",
       "              ('człowieka', 0.4325554983507145),\n",
       "              ('krawatach', 0.4454413330731455),\n",
       "              ('powody', 0.4469661759184015)],\n",
       "             'potrafi': [('usprawiedliwienie', 0.5316574419491084),\n",
       "              ('usłyszeć', 0.5465419664365483),\n",
       "              ('pochwały', 0.5729732027822341),\n",
       "              ('dorosłemu', 0.600687459087724)],\n",
       "             'wszystko': [('miesza', 0.5747617535688968),\n",
       "              ('przyjemność', 0.5953941120551371),\n",
       "              ('mimo', 0.6096148219102052),\n",
       "              ('przeliczyli', 0.6408985918467736)],\n",
       "             'zrozumieć': [('błahostkę', 0.5295959366860877),\n",
       "              ('zrozumie', 0.5345679409638675),\n",
       "              ('książkę', 0.5357375870741634),\n",
       "              ('wspomnień', 0.5479746415232992)],\n",
       "             'książki': [('trzecie', 0.4267128537212571),\n",
       "              ('grube', 0.48762708766592866),\n",
       "              ('pisał', 0.5222664976928375),\n",
       "              ('dzieci', 0.5275103699119057)],\n",
       "             'dzieci': [('ziewają', 0.40831107564370117),\n",
       "              ('trzecie', 0.4639159109060659),\n",
       "              ('usprawiedliwienie', 0.4885943151884439),\n",
       "              ('szybach', 0.4921509053288705)]})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from collections import defaultdict\n",
    "\n",
    "vecs = np.zeros((MAX_FEATURES, EMBEDDING_DIM))\n",
    "for i in range(1, MAX_FEATURES):\n",
    "    vecs[i] = word2vec[index_word[i]]\n",
    "    \n",
    "closest = defaultdict(list)\n",
    "for idx in test_words_idx:\n",
    "    dists = pairwise_distances(vecs, metric='cosine')\n",
    "    closest_id = np.argsort(dists[idx, :])[1:5]\n",
    "    for i in closest_id:\n",
    "        closest[index_word[idx]].append((index_word[i], dists[idx, i]))\n",
    "closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('word2vec.h5', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
